===== Training step 1 =====
Parameters: {'lambd': 5.928446182250184, 'rho': 8.442657485810175}
Total training relative RMSE: 4.999227665398313
===== Training step 2 =====
Parameters: {'lambd': 8.57945617622757, 'rho': 8.472517387841256}
Total training relative RMSE: 4.998799672453776
===== Training step 3 =====
Parameters: {'lambd': 6.235636967859724, 'rho': 3.843817072926999}
Total training relative RMSE: 4.999201268545414
===== Training step 4 =====
Parameters: {'lambd': 2.9753460654447235, 'rho': 0.5671297731744319}
Total training relative RMSE: 2.449800273198739
===== Training step 5 =====
Parameters: {'lambd': 2.7265629458011325, 'rho': 4.7766511732135}
Total training relative RMSE: 5.000187413850602
===== Training step 6 =====
Parameters: {'lambd': 8.121687287754932, 'rho': 4.799771723750574}
Total training relative RMSE: 4.998833760465818
===== Training step 7 =====
Parameters: {'lambd': 3.927847961008298, 'rho': 8.360787635373777}
Total training relative RMSE: 4.999722202718166
===== Training step 8 =====
Parameters: {'lambd': 3.3739616041726843, 'rho': 6.4817187205119735}
Total training relative RMSE: 4.999921475255649
===== Training step 9 =====
Parameters: {'lambd': 3.6824153984054804, 'rho': 9.571551589530465}
Total training relative RMSE: 4.999794272726711
===== Training step 10 =====
Parameters: {'lambd': 1.4035078041264517, 'rho': 8.700872583584365}
Total training relative RMSE: 5.000409178011072
===== Training step 11 =====
Parameters: {'lambd': 1.0590760718779215, 'rho': 3.1218988470880276}
Total training relative RMSE: 5.000471209711154
===== Training step 12 =====
Parameters: {'lambd': 3.0016836846745814, 'rho': 1.2505440660003355}
Total training relative RMSE: 4.998995873391459
===== Training step 13 =====
Parameters: {'lambd': 3.0727341109573554, 'rho': 3.1426235112366236}
Total training relative RMSE: 5.00011480878687
===== Training step 14 =====
Parameters: {'lambd': 3.0574463812677704, 'rho': 0.7474127586203695}
Total training relative RMSE: 2.8648263691841835
===== Training step 15 =====
Parameters: {'lambd': 2.4483612495180034, 'rho': 0.30681410445203344}
Total training relative RMSE: 2.644198335348534
===== Training step 16 =====
Parameters: {'lambd': 2.9956522378421586, 'rho': 0.4320123063380921}
Total training relative RMSE: 2.5286091039466214
===== Training step 17 =====
Parameters: {'lambd': 2.6587895261296106, 'rho': 0.36715254925117186}
Total training relative RMSE: 2.6022965891154146
===== Training step 18 =====
Parameters: {'lambd': 2.896321873287899, 'rho': 0.004576132068127015}
Total training relative RMSE: 2.6682855975896858
===== Training step 19 =====
Parameters: {'lambd': 2.97269259912911, 'rho': 0.20347045450284765}
Total training relative RMSE: 2.6230329383036306
===== Training step 20 =====
Parameters: {'lambd': 2.9468749881576235, 'rho': 0.39865608104343037}
Total training relative RMSE: 2.554873266427805
===== Training step 21 =====
Parameters: {'lambd': 2.973247103458783, 'rho': 0.48597907220686587}
Total training relative RMSE: 2.496721059071023
===== Training step 22 =====
Parameters: {'lambd': 2.9760115320609954, 'rho': 0.5585352122548749}
Total training relative RMSE: 2.453893965624257
===== Training step 23 =====
Parameters: {'lambd': 2.964987616553621, 'rho': 0.5296093092311983}
Total training relative RMSE: 2.4698030629479613
===== Training step 24 =====
Parameters: {'lambd': 3.0879519913239277, 'rho': 0.5393143007753421}
Total training relative RMSE: 2.4562264787354464
===== Training step 25 =====
Parameters: {'lambd': 3.7154185421539103, 'rho': 0.5297594622181346}
Total training relative RMSE: 2.4713626000583426
===== Training step 26 =====
Parameters: {'lambd': 4.336000301972183, 'rho': 0.5437074612327409}
Total training relative RMSE: 2.656980235060333
===== Training step 27 =====
Parameters: {'lambd': 5.818411488889756, 'rho': 0.5309302589884392}
Total training relative RMSE: 3.2560500766217193
===== Training step 28 =====
Parameters: {'lambd': 2.907941652727539, 'rho': 0.7345513834661267}
Total training relative RMSE: 2.6949373980745244
===== Training step 29 =====
Parameters: {'lambd': 1.181153407759329, 'rho': 0.5781170147161175}
Total training relative RMSE: 2.6853868687951965
===== Training step 30 =====
Parameters: {'lambd': 1.0387424070493247, 'rho': 0.6135256219158126}
Total training relative RMSE: 2.690652772012995
===== Training step 31 =====
Parameters: {'lambd': 2.968820050900422, 'rho': 0.8982264079583059}
Total training relative RMSE: 4.31557454478648
===== Training step 32 =====
Parameters: {'lambd': 0.721161304789475, 'rho': 0.5462708933328376}
Total training relative RMSE: 2.7668785643892075
===== Training step 33 =====
Parameters: {'lambd': 6.499440392917398, 'rho': 0.43262845928601085}
Total training relative RMSE: 3.100764615250552
===== Training step 34 =====
Parameters: {'lambd': 2.850788640414554, 'rho': 0.6001535433760464}
Total training relative RMSE: 2.442119365550226
===== Training step 35 =====
Parameters: {'lambd': 2.9393800476062464, 'rho': 0.5753666559362992}
Total training relative RMSE: 2.4479741958137984
===== Training step 36 =====
Parameters: {'lambd': 2.8684437063582706, 'rho': 0.6058969436178997}
Total training relative RMSE: 2.4404140769230587
===== Training step 37 =====
Parameters: {'lambd': 2.9327672521436883, 'rho': 0.6333148926297129}
Total training relative RMSE: 2.4468871612829277
===== Training step 38 =====
Parameters: {'lambd': 2.8818478398949625, 'rho': 0.629473582419109}
Total training relative RMSE: 2.441120750687982
===== Training step 39 =====
Parameters: {'lambd': 2.984442616365626, 'rho': 0.6288644361466235}
Total training relative RMSE: 2.4501543843642084
===== Training step 40 =====
Parameters: {'lambd': 2.613946566786683, 'rho': 0.6811603515554377}
Total training relative RMSE: 2.4365918608495174
===== Training step 41 =====
Parameters: {'lambd': 2.9796007416126873, 'rho': 0.6635978988070502}
Total training relative RMSE: 2.4963718113343485
===== Training step 42 =====
Parameters: {'lambd': 3.033236909374914, 'rho': 0.6644543072383858}
Total training relative RMSE: 2.5138503344135534
===== Training step 43 =====
Parameters: {'lambd': 2.7272975578534715, 'rho': 0.6977964331008947}
Total training relative RMSE: 2.488751529267434
===== Training step 44 =====
Parameters: {'lambd': 3.0320604229590735, 'rho': 0.6695738157091148}
Total training relative RMSE: 2.526756945419644
===== Training step 45 =====
Parameters: {'lambd': 3.0016693388973463, 'rho': 0.6793957933014417}
Total training relative RMSE: 2.5410101152099736
===== Training step 46 =====
Parameters: {'lambd': 2.7647350492293272, 'rho': 0.8931500176192398}
Total training relative RMSE: 4.040987708370763
===== Training step 47 =====
Parameters: {'lambd': 2.603557031459211, 'rho': 0.6815233368036201}
Total training relative RMSE: 2.435533088203106
===== Training step 48 =====
Parameters: {'lambd': 2.6932157832107633, 'rho': 0.6752465514005391}
Total training relative RMSE: 2.4449793803259543
===== Training step 49 =====
Parameters: {'lambd': 2.531241544736113, 'rho': 0.6774242170888113}
Total training relative RMSE: 2.4270013227349927
===== Training step 50 =====
Parameters: {'lambd': 2.357954353655848, 'rho': 0.6699701024642691}
Total training relative RMSE: 2.432402880011144
===== Training step 51 =====
Parameters: {'lambd': 2.331928621799987, 'rho': 0.6683546626145421}
Total training relative RMSE: 2.4355516776545105
===== Training step 52 =====
Parameters: {'lambd': 2.547063253033251, 'rho': 0.7066251491089371}
Total training relative RMSE: 2.4531931015709953
===== Training step 53 =====
Parameters: {'lambd': 2.5086294538500793, 'rho': 0.8008944125549845}
Total training relative RMSE: 2.787284879214921
===== Training step 54 =====
Parameters: {'lambd': 1.2574017320210442, 'rho': 0.6703734537083618}
Total training relative RMSE: 2.628573805113872
===== Training step 55 =====
Parameters: {'lambd': 2.7623203316087612, 'rho': 0.64305636223712}
Total training relative RMSE: 2.4358437966410276
===== Training step 56 =====
Parameters: {'lambd': 3.045012407427781, 'rho': 0.6883176300034922}
Total training relative RMSE: 2.584317429195881
===== Training step 57 =====
Parameters: {'lambd': 0.30832749600347104, 'rho': 0.6772862684202031}
Total training relative RMSE: 2.7843002119640436
===== Training step 58 =====
Parameters: {'lambd': 2.8288767944817965, 'rho': 0.659405417003145}
Total training relative RMSE: 2.453230482228848
===== Training step 59 =====
Parameters: {'lambd': 2.981382608989484, 'rho': 0.679443721119415}
Total training relative RMSE: 2.5333518223919302
===== Training step 60 =====
Parameters: {'lambd': 2.7406309812621736, 'rho': 0.6850303763753042}
Total training relative RMSE: 2.4690756272395387
===== Training step 61 =====
Parameters: {'lambd': 2.955126533217087, 'rho': 0.6751279858057325}
Total training relative RMSE: 2.512314431254128
===== Training step 62 =====
Parameters: {'lambd': 2.7715741563641245, 'rho': 0.6642144269494233}
Total training relative RMSE: 2.4477726656117023
===== Training step 63 =====
Parameters: {'lambd': 2.7371725208640934, 'rho': 0.6611746082885529}
Total training relative RMSE: 2.439789843590401
===== Training step 64 =====
Parameters: {'lambd': 3.1180720593537576, 'rho': 0.7250013091250119}
Total training relative RMSE: 2.7776593860794776
===== Training step 65 =====
Parameters: {'lambd': 2.607304631893263, 'rho': 0.663286520620976}
Total training relative RMSE: 2.4303407257062783
===== Training step 66 =====
Parameters: {'lambd': 2.3173204436310106, 'rho': 0.7526291081989868}
Total training relative RMSE: 2.4682055888992527
===== Training step 67 =====
Parameters: {'lambd': 2.304080883602935, 'rho': 0.7759088179734476}
Total training relative RMSE: 2.5254157138296516
===== Training step 68 =====
Parameters: {'lambd': 2.3095071580783935, 'rho': 0.8473206734757012}
Total training relative RMSE: 2.982000799200476
===== Training step 69 =====
Parameters: {'lambd': 2.99541318646076, 'rho': 0.7875004640193296}
Total training relative RMSE: 3.1143004714057287
===== Training step 70 =====
Parameters: {'lambd': 2.6566307813290853, 'rho': 0.6619228166410575}
Total training relative RMSE: 2.4323173473948594
===== Training step 71 =====
Parameters: {'lambd': 2.762778860825083, 'rho': 0.6049685673033179}
Total training relative RMSE: 2.4437931457146873
===== Training step 72 =====
Parameters: {'lambd': 2.8775649723743495, 'rho': 0.6435988242462688}
Total training relative RMSE: 2.4463088973600353
===== Training step 73 =====
Parameters: {'lambd': 3.0489759087578543, 'rho': 0.6453856715544016}
Total training relative RMSE: 2.482893790482559
===== Training step 74 =====
Parameters: {'lambd': 2.7741087671819815, 'rho': 0.6859641864608158}
Total training relative RMSE: 2.479872649895013
===== Training step 75 =====
Parameters: {'lambd': 3.0684939370834456, 'rho': 0.6380971904315059}
Total training relative RMSE: 2.476403162951833
===== Training step 76 =====
Parameters: {'lambd': 1.6522298358555823, 'rho': 1.044888179339364}
Total training relative RMSE: 4.897296994647795
===== Training step 77 =====
Parameters: {'lambd': 1.966162947146493, 'rho': 0.6191833544278792}
Total training relative RMSE: 2.5432829632172482
===== Training step 78 =====
Parameters: {'lambd': 1.9026032305395184, 'rho': 0.6642873172730669}
Total training relative RMSE: 2.5060042999319085
===== Training step 79 =====
Parameters: {'lambd': 1.9954843754129474, 'rho': 0.660587219730905}
Total training relative RMSE: 2.492023002556447
===== Training step 80 =====
Parameters: {'lambd': 1.7218227166537994, 'rho': 0.6716483158801835}
Total training relative RMSE: 2.5353311072144677
===== Training step 81 =====
Parameters: {'lambd': 1.7714968539526401, 'rho': 0.7723294429690876}
Total training relative RMSE: 2.3920994210918907
===== Training step 82 =====
Parameters: {'lambd': 1.8981174675115602, 'rho': 0.7984928072550147}
Total training relative RMSE: 2.413724602932521
===== Training step 83 =====
Parameters: {'lambd': 1.7539797520329183, 'rho': 0.7822908419965448}
Total training relative RMSE: 2.383086815019796
===== Training step 84 =====
Parameters: {'lambd': 1.6578442879123003, 'rho': 0.8160911710353981}
Total training relative RMSE: 2.367992958154224
===== Training step 85 =====
Parameters: {'lambd': 1.5810046826152915, 'rho': 0.7976637159569945}
Total training relative RMSE: 2.379715639632464
===== Training step 86 =====
Parameters: {'lambd': 1.5778663823505825, 'rho': 0.8218595140152031}
Total training relative RMSE: 2.3573986734859504
===== Training step 87 =====
Parameters: {'lambd': 1.6217763933567455, 'rho': 0.8296948994629217}
Total training relative RMSE: 2.370255526300425
===== Training step 88 =====
Parameters: {'lambd': 1.585346637614066, 'rho': 0.841415994867677}
Total training relative RMSE: 2.37250116927984
===== Training step 89 =====
Parameters: {'lambd': 1.563737439150962, 'rho': 0.9325891605468385}
Total training relative RMSE: 2.994505611852067
===== Training step 90 =====
Parameters: {'lambd': 1.526845007588076, 'rho': 0.8508869338031412}
Total training relative RMSE: 2.3643183351485177
===== Training step 91 =====
Parameters: {'lambd': 1.7238334284315884, 'rho': 0.8958810232798954}
Total training relative RMSE: 2.7735034356863597
===== Training step 92 =====
Parameters: {'lambd': 1.6554116194745063, 'rho': 0.8680316945440049}
Total training relative RMSE: 2.4889145532388266
===== Training step 93 =====
Parameters: {'lambd': 1.5919839296880403, 'rho': 0.9964447644812571}
Total training relative RMSE: 4.334881268373779
===== Training step 94 =====
Parameters: {'lambd': 1.648431940283981, 'rho': 0.8181179640652981}
Total training relative RMSE: 2.3669807224705237
===== Training step 95 =====
Parameters: {'lambd': 1.6514474390169223, 'rho': 0.8325326415476598}
Total training relative RMSE: 2.383229214735761
===== Training step 96 =====
Parameters: {'lambd': 1.1515529642474045, 'rho': 0.8228965136948675}
Total training relative RMSE: 2.429818726600097
===== Training step 97 =====
Parameters: {'lambd': 0.7842738969565456, 'rho': 0.82779286987972}
Total training relative RMSE: 2.5680486384501235
===== Training step 98 =====
Parameters: {'lambd': 1.049967924774278, 'rho': 0.8116253695800936}
Total training relative RMSE: 2.4973551658826727
===== Training step 99 =====
Parameters: {'lambd': 1.6345628658993396, 'rho': 0.8244491232561081}
Total training relative RMSE: 2.3681427239735187
===== Training step 100 =====
Parameters: {'lambd': 1.4567807817862812, 'rho': 0.831721891256111}
Total training relative RMSE: 2.342070736279585


Best training relative RMSE: 2.3421
Best training parameters: [1.4567807817862812, 0.831721891256111]
