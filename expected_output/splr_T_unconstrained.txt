===== Training step 1 =====
Parameters: {'lambd': 5.928446182250184, 'rho': 8.442657485810175}
Total training relative RMSE: 4.999449023971332
===== Training step 2 =====
Parameters: {'lambd': 8.57945617622757, 'rho': 8.472517387841256}
Total training relative RMSE: 4.9995991781696345
===== Training step 3 =====
Parameters: {'lambd': 6.235636967859724, 'rho': 3.843817072926999}
Total training relative RMSE: 4.999320310081378
===== Training step 4 =====
Parameters: {'lambd': 2.9753460654447235, 'rho': 0.5671297731744319}
Total training relative RMSE: 2.4509945398007567
===== Training step 5 =====
Parameters: {'lambd': 2.7265629458011325, 'rho': 4.7766511732135}
Total training relative RMSE: 5.000494273737271
===== Training step 6 =====
Parameters: {'lambd': 8.121687287754932, 'rho': 4.799771723750574}
Total training relative RMSE: 4.999499891866465
===== Training step 7 =====
Parameters: {'lambd': 3.927847961008298, 'rho': 8.360787635373777}
Total training relative RMSE: 5.00003275327048
===== Training step 8 =====
Parameters: {'lambd': 3.3739616041726843, 'rho': 6.4817187205119735}
Total training relative RMSE: 5.000264934136467
===== Training step 9 =====
Parameters: {'lambd': 3.6824153984054804, 'rho': 9.571551589530465}
Total training relative RMSE: 5.000155683088288
===== Training step 10 =====
Parameters: {'lambd': 1.4035078041264517, 'rho': 8.700872583584365}
Total training relative RMSE: 5.000712640778692
===== Training step 11 =====
Parameters: {'lambd': 1.0590760718779215, 'rho': 3.1218988470880276}
Total training relative RMSE: 5.000714986693203
===== Training step 12 =====
Parameters: {'lambd': 3.0016836846745814, 'rho': 1.2505440660003355}
Total training relative RMSE: 4.999362012623708
===== Training step 13 =====
Parameters: {'lambd': 3.0727341109573554, 'rho': 3.1426235112366236}
Total training relative RMSE: 5.000397207630531
===== Training step 14 =====
Parameters: {'lambd': 3.0574463812677704, 'rho': 0.7474127586203695}
Total training relative RMSE: 3.3796231755498427
===== Training step 15 =====
Parameters: {'lambd': 2.4483612495180034, 'rho': 0.30681410445203344}
Total training relative RMSE: 2.6347776350892413
===== Training step 16 =====
Parameters: {'lambd': 2.5842718065663544, 'rho': 0.36218687091347906}
Total training relative RMSE: 2.6120163033345962
===== Training step 17 =====
Parameters: {'lambd': 1.4886313859653335, 'rho': 0.24522571825855272}
Total training relative RMSE: 2.6488731237892766
===== Training step 18 =====
Parameters: {'lambd': 0.6742652611180645, 'rho': 0.3273795869553054}
Total training relative RMSE: 2.649861259364301
===== Training step 19 =====
Parameters: {'lambd': 1.4367813328073278, 'rho': 0.3810483617761696}
Total training relative RMSE: 2.649835340855377
===== Training step 20 =====
Parameters: {'lambd': 0.0673469960890405, 'rho': 0.5122210017304297}
Total training relative RMSE: 2.6486034144386217
===== Training step 21 =====
Parameters: {'lambd': 2.3619780573235865, 'rho': 0.24990447273430477}
Total training relative RMSE: 2.6446305120647584
===== Training step 22 =====
Parameters: {'lambd': 2.737362597714295, 'rho': 0.39846374526829825}
Total training relative RMSE: 2.5821918387476983
===== Training step 23 =====
Parameters: {'lambd': 2.76297341379619, 'rho': 0.3679883901007686}
Total training relative RMSE: 2.5936161971502076
===== Training step 24 =====
Parameters: {'lambd': 2.9543588060989405, 'rho': 0.3638889404678614}
Total training relative RMSE: 2.5763544427387286
===== Training step 25 =====
Parameters: {'lambd': 4.303949534061003, 'rho': 0.323060850635244}
Total training relative RMSE: 2.4817282258414584
===== Training step 26 =====
Parameters: {'lambd': 6.30920997607931, 'rho': 0.3208406543264342}
Total training relative RMSE: 2.7861572209856083
===== Training step 27 =====
Parameters: {'lambd': 7.563598862152737, 'rho': 0.3848676934823148}
Total training relative RMSE: 3.684927127025729
===== Training step 28 =====
Parameters: {'lambd': 0.6475749826353773, 'rho': 0.7699816804865746}
Total training relative RMSE: 2.6456630280916458
===== Training step 29 =====
Parameters: {'lambd': 0.7506518918709172, 'rho': 0.6901066586707684}
Total training relative RMSE: 2.6480604115825717
===== Training step 30 =====
Parameters: {'lambd': 0.3624585932564251, 'rho': 0.8948568502944155}
Total training relative RMSE: 2.6224465571674878
===== Training step 31 =====
Parameters: {'lambd': 0.1221908583554521, 'rho': 0.9716252346856049}
Total training relative RMSE: 2.36246037338247
===== Training step 32 =====
Parameters: {'lambd': 0.11552143883801326, 'rho': 1.0644989608200863}
Total training relative RMSE: 3.908887552916152
===== Training step 33 =====
Parameters: {'lambd': 0.5996105878724035, 'rho': 0.906073556425301}
Total training relative RMSE: 2.3425792946947195
===== Training step 34 =====
Parameters: {'lambd': 0.6380023480891185, 'rho': 0.9392601844633732}
Total training relative RMSE: 2.0852205374428365
===== Training step 35 =====
Parameters: {'lambd': 0.6976671511418732, 'rho': 0.9669444385013051}
Total training relative RMSE: 2.964499445080178
===== Training step 36 =====
Parameters: {'lambd': 1.5376973497022386, 'rho': 0.9383301472132313}
Total training relative RMSE: 4.308615226487183
===== Training step 37 =====
Parameters: {'lambd': 0.5945382003525347, 'rho': 0.9392159773284527}
Total training relative RMSE: 2.055746606235004
===== Training step 38 =====
Parameters: {'lambd': 0.5895782760520075, 'rho': 1.258938497376825}
Total training relative RMSE: 5.000987083432058
===== Training step 39 =====
Parameters: {'lambd': 0.5846579439769574, 'rho': 0.9489152738343201}
Total training relative RMSE: 2.075248071165081
===== Training step 40 =====
Parameters: {'lambd': 0.6038936691780018, 'rho': 0.9285308987976483}
Total training relative RMSE: 2.116834171619588
===== Training step 41 =====
Parameters: {'lambd': 0.6416924123542279, 'rho': 0.9870086594376173}
Total training relative RMSE: 3.4129238156444215
===== Training step 42 =====
Parameters: {'lambd': 0.5076753231043364, 'rho': 0.9496360210068524}
Total training relative RMSE: 1.999294087703758
===== Training step 43 =====
Parameters: {'lambd': 0.5649542764652616, 'rho': 0.9431862596391496}
Total training relative RMSE: 2.0365837997027683
===== Training step 44 =====
Parameters: {'lambd': 0.8703672462273805, 'rho': 0.9448005360909608}
Total training relative RMSE: 2.8044850491948634
===== Training step 45 =====
Parameters: {'lambd': 0.5457139821887437, 'rho': 0.8671318466450563}
Total training relative RMSE: 2.5952940987983304
===== Training step 46 =====
Parameters: {'lambd': 0.5468384306410868, 'rho': 0.9522525516695304}
Total training relative RMSE: 2.043899086056427
===== Training step 47 =====
Parameters: {'lambd': 0.5843519509978113, 'rho': 1.0474904702146006}
Total training relative RMSE: 4.501220862563685
===== Training step 48 =====
Parameters: {'lambd': 0.5305446077528199, 'rho': 0.9469036713476988}
Total training relative RMSE: 2.0149656710869883
===== Training step 49 =====
Parameters: {'lambd': 0.47527289449903154, 'rho': 0.9567202426798318}
Total training relative RMSE: 1.9751641006800384
===== Training step 50 =====
Parameters: {'lambd': 0.5690707510081429, 'rho': 0.9078866616932547}
Total training relative RMSE: 2.363103206230649
===== Training step 51 =====
Parameters: {'lambd': 0.42076197239193097, 'rho': 0.9553647314642302}
Total training relative RMSE: 1.9650600473610265
===== Training step 52 =====
Parameters: {'lambd': 0.5422804521933678, 'rho': 0.9541268262177462}
Total training relative RMSE: 2.0541666948595045
===== Training step 53 =====
Parameters: {'lambd': 0.40408777109642086, 'rho': 0.972724762411227}
Total training relative RMSE: 2.0402340971593467
===== Training step 54 =====
Parameters: {'lambd': 0.5595363707983648, 'rho': 0.9515437640073845}
Total training relative RMSE: 2.058173856122047
===== Training step 55 =====
Parameters: {'lambd': 0.5109513487189977, 'rho': 1.009370975603119}
Total training relative RMSE: 3.6774959124815636
===== Training step 56 =====
Parameters: {'lambd': 0.48465516997149083, 'rho': 0.9806812179119331}
Total training relative RMSE: 2.6796199734819073
===== Training step 57 =====
Parameters: {'lambd': 0.5528406172776325, 'rho': 0.9516988587756983}
Total training relative RMSE: 2.0487106205539956
===== Training step 58 =====
Parameters: {'lambd': 0.3432703179261943, 'rho': 0.9519074453768264}
Total training relative RMSE: 2.1376660419120075
===== Training step 59 =====
Parameters: {'lambd': 0.588555959898951, 'rho': 0.9425984529372624}
Total training relative RMSE: 2.0502612219641496
===== Training step 60 =====
Parameters: {'lambd': 0.5531612608690973, 'rho': 0.9417530772062723}
Total training relative RMSE: 2.038783180722057
===== Training step 61 =====
Parameters: {'lambd': 0.581982223582267, 'rho': 0.9679865310762172}
Total training relative RMSE: 2.5650301361818193
===== Training step 62 =====
Parameters: {'lambd': 0.38599299050465447, 'rho': 0.9427563364230487}
Total training relative RMSE: 2.2170398486944674
===== Training step 63 =====
Parameters: {'lambd': 0.5454319194395097, 'rho': 0.9314696964128079}
Total training relative RMSE: 2.1355716560146707
===== Training step 64 =====
Parameters: {'lambd': 0.5431875394235509, 'rho': 0.9614391251555379}
Total training relative RMSE: 2.194190058451016
===== Training step 65 =====
Parameters: {'lambd': 0.5828967966606292, 'rho': 0.9175813684056345}
Total training relative RMSE: 2.2481120553056253
===== Training step 66 =====
Parameters: {'lambd': 0.42762559249897586, 'rho': 0.9477027574080346}
Total training relative RMSE: 2.0596697655145837
===== Training step 67 =====
Parameters: {'lambd': 0.4356494424963398, 'rho': 1.0643005830524501}
Total training relative RMSE: 4.469286892693819
===== Training step 68 =====
Parameters: {'lambd': 0.5071120628170135, 'rho': 0.9559009104739371}
Total training relative RMSE: 2.0114318087487333
===== Training step 69 =====
Parameters: {'lambd': 0.4663029381029327, 'rho': 0.9543639473701628}
Total training relative RMSE: 1.9699594714846804
===== Training step 70 =====
Parameters: {'lambd': 0.4182323714209691, 'rho': 0.970611068201789}
Total training relative RMSE: 2.0315995748169215
===== Training step 71 =====
Parameters: {'lambd': 0.4081348911690264, 'rho': 0.9878676844479997}
Total training relative RMSE: 2.664054006378157
===== Training step 72 =====
Parameters: {'lambd': 0.4217426481914533, 'rho': 0.9910405235929056}
Total training relative RMSE: 2.8481004254065105
===== Training step 73 =====
Parameters: {'lambd': 0.4982548158286383, 'rho': 0.9425187110727553}
Total training relative RMSE: 2.0530332085149174
===== Training step 74 =====
Parameters: {'lambd': 0.5023234545529499, 'rho': 0.952620601907125}
Total training relative RMSE: 1.9930123597163854
===== Training step 75 =====
Parameters: {'lambd': 0.5128293557800491, 'rho': 0.953869754373813}
Total training relative RMSE: 2.006941026302749
===== Training step 76 =====
Parameters: {'lambd': 0.435711906595514, 'rho': 0.9232287349301272}
Total training relative RMSE: 2.397720314546719
===== Training step 77 =====
Parameters: {'lambd': 0.6036517153778477, 'rho': 0.9612495550903968}
Total training relative RMSE: 2.401562985304052
===== Training step 78 =====
Parameters: {'lambd': 0.5348065365773892, 'rho': 1.1850838829441148}
Total training relative RMSE: 4.984048377029997
===== Training step 79 =====
Parameters: {'lambd': 0.40937953678821587, 'rho': 0.9729933013923111}
Total training relative RMSE: 2.0657548193765614
===== Training step 80 =====
Parameters: {'lambd': 0.5277873926120625, 'rho': 0.9475699685580476}
Total training relative RMSE: 2.01232192044536
===== Training step 81 =====
Parameters: {'lambd': 0.492227203775243, 'rho': 0.9370303221524857}
Total training relative RMSE: 2.1280235460181585
===== Training step 82 =====
Parameters: {'lambd': 0.5204293563083074, 'rho': 0.9674890642518332}
Total training relative RMSE: 2.3004654006430614
===== Training step 83 =====
Parameters: {'lambd': 0.852567259582654, 'rho': 0.9280426112316321}
Total training relative RMSE: 2.319480005979897
===== Training step 84 =====
Parameters: {'lambd': 0.544196829062591, 'rho': 0.950780736077246}
Total training relative RMSE: 2.030370545553091
===== Training step 85 =====
Parameters: {'lambd': 0.6176375888205855, 'rho': 0.9365073768476818}
Total training relative RMSE: 2.0693740189013843
===== Training step 86 =====
Parameters: {'lambd': 0.4652814830723096, 'rho': 0.962711600779387}
Total training relative RMSE: 2.0064098920193354
===== Training step 87 =====
Parameters: {'lambd': 0.5949217792067208, 'rho': 0.9668251949931741}
Total training relative RMSE: 2.571892992965538
===== Training step 88 =====
Parameters: {'lambd': 0.4492618404684479, 'rho': 0.9472864642084158}
Total training relative RMSE: 2.039449893896857
===== Training step 89 =====
Parameters: {'lambd': 0.5294667756242301, 'rho': 0.9586626069014383}
Total training relative RMSE: 2.091865128573207
===== Training step 90 =====
Parameters: {'lambd': 0.47060742360949825, 'rho': 0.9466469456425577}
Total training relative RMSE: 2.028152329310561
===== Training step 91 =====
Parameters: {'lambd': 0.5611648388972669, 'rho': 0.9805873982684989}
Total training relative RMSE: 2.969369377522478
===== Training step 92 =====
Parameters: {'lambd': 0.4431042790819207, 'rho': 0.9717215492029676}
Total training relative RMSE: 2.148584579292932
===== Training step 93 =====
Parameters: {'lambd': 0.4075814263081368, 'rho': 0.9998662474770372}
Total training relative RMSE: 3.123040270549577
===== Training step 94 =====
Parameters: {'lambd': 0.5410934775769173, 'rho': 0.9797747091769717}
Total training relative RMSE: 2.864742286656559
===== Training step 95 =====
Parameters: {'lambd': 0.5576399609968942, 'rho': 0.9539425523766367}
Total training relative RMSE: 2.082097019560748
===== Training step 96 =====
Parameters: {'lambd': 0.47203124290332144, 'rho': 0.972100243377856}
Total training relative RMSE: 2.274988828956187
===== Training step 97 =====
Parameters: {'lambd': 0.4724082775308459, 'rho': 1.020249729964802}
Total training relative RMSE: 3.8488336367604887
===== Training step 98 =====
Parameters: {'lambd': 0.5388728181943893, 'rho': 0.9367068791709666}
Total training relative RMSE: 2.083840482399139
===== Training step 99 =====
Parameters: {'lambd': 0.5263704049677044, 'rho': 0.9053510504411267}
Total training relative RMSE: 2.436495470766473
===== Training step 100 =====
Parameters: {'lambd': 0.5295297651846554, 'rho': 0.9397263812049885}
Total training relative RMSE: 2.060458989683356


Best training relative RMSE: 1.9651
Best training parameters: [0.42076197239193097, 0.9553647314642302]
